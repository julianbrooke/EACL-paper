%
% File acl2012.tex
%
% Contact: Maggie Li (cswjli@comp.polyu.edu.hk), Michael White (mwhite@ling.osu.edu)
%%
%% Based on the style files for ACL2008 by Joakim Nivre and Noah Smith
%% and that of ACL2010 by Jing-Shin Chang and Philipp Koehn


\documentclass[11pt,letterpaper]{article}
\usepackage[letterpaper]{geometry}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\usepackage{xspace}
\usepackage{paralist}
\usepackage{booktabs}
\usepackage[pdftex]{graphicx}
\usepackage{algorithm,algorithmicx}
\usepackage[noend]{algpseudocode}
%\usepackage{luatexja-fontspec}
%\setmainjfont{MS Mincho} % \mcfamily
%\setsansjfont{MS Gothic} % \gtfamily
%\usepackage{xeCJK}
%\setCJKmainfont{MS Mincho} % for \rmfamily
%\setCJKsansfont{MS Gothic} % for \sffamily
\usepackage{CJKutf8}
\makeatletter
\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
\usepackage[hidelinks]{hyperref}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\def \eg {e.g.,\@ }
\def \ie {i.e.,\@ }
\def \al {al.\@ }
\def \etc {etc.\@ }


\newcommand\BibTeX{B{\sc ib}\TeX}

% TJB macros
\newcommand{\dotts}{...\xspace}
\newcommand{\gap}{$*$\xspace}
\newcommand{\z}{\phantom{0}}
\newcommand{\ex}[1]{\textit{#1}\xspace}
\newcommand{\gl}[1]{``#1''\xspace}
\newcommand{\termdef}[1]{\textbf{#1}\xspace}
\newcommand{\termemph}[1]{\textit{#1}\xspace}

\newcommand{\LPR}{\ensuremath{\text{LPR}}}
\newcommand{\minLPR}{\ensuremath{\text{minLPR}}}
\newcommand{\cover}{\ensuremath{\text{cover}}\xspace}
\newcommand{\clear}{\ensuremath{\text{clear}}\xspace}
\newcommand{\overlap}{\ensuremath{\text{overlap}}\xspace}
\newcommand{\expl}{\ensuremath{\text{expl}}\xspace}

\newcommand{\figref}[2][]{Figure#1~\ref{#2}\xspace}
\newcommand{\tabref}[2][]{Table#1~\ref{#2}\xspace}
\newcommand{\secref}[2][]{Section#1~\ref{#2}\xspace}
\newcommand{\algoref}[2][]{Algorithm#1~\ref{#2}\xspace}


\title{Unsupervised Acquisition of Comprehensive Multiword Lexicons \\ using Competition in an $n$-gram Lattice}


\author{
Julian Brooke$^{*}$ \quad Jan \v{S}najder$^{\dagger}$ \quad Timothy Baldwin$^{*}$ \\
$^{*}$\,School of Computing and Information Systems, The University of Melbourne\\
\texttt{julian.brooke@gmail.com, tb@ldwin.net} \\
$^{\dagger}$\,Faculty of Electrical Engineering and Computing, University
of Zagreb \\
\texttt{jan.snajder@fer.hr}\\
}

%      \author{Julian Brooke \qquad Timothy Baldwin \\ Computing and Information Systems \\ The University of Melbourne \\ \url{julian.brooke@gmail.com} \\ \url{tb@ldwin.net}
% \And               Jan Snajder\\
%Electrical Engineering and Computing \\
% University of Zagreb \\
%  {\tt jan.snajder@fer.hr}}

\date{}

\begin{document}
\maketitle


\begin{abstract}
We present a new model for acquiring comprehensive multiword lexicons from large corpora based on  competition among $n$-gram candidates. In contrast to the standard approach of simple ranking by association measure, in our model $n$-grams are arranged in a lattice structure based on subsumption and overlap relationships, with nodes inhibiting other nodes in their vicinity when they are selected as a lexical item. We show how the configuration of such a lattice can be optimized tractably, and demonstrate using annotations of sampled $n$-grams that our method consistently outperforms alternatives by at least 0.05 F-score across several corpora and languages.
\end{abstract}


\section{Introduction}

Despite over 25 years of research in computational linguistics aimed at acquiring multiword lexicons using corpora statistics, and growing evidence that speakers process language primarily in terms of memorized sequences \cite{Wray08}, the individual word nonetheless stubbornly remains the \textit{de facto} standard processing unit for most research in modern NLP. The potential of multiword knowledge to improve both the automatic processing of language as well as offer new understanding of human acquisition and usage of language is the primary motivator of this work. Here, we present an effective, expandable, and tractable new approach to comprehensive multiword lexicon acquisition. Our aim is to find a middle ground between standard MWE acquisition approaches based on association measures \cite{Ramisch14} and more sophisticated statistical models \cite{Newman12} that do not scale to large corpora, the main source of the distributional information in modern NLP systems.


%Methods based on association measures tend to assume a small, fixed set of part-of-speech (``POS'') combinations with higher productivity \cite{Evert:Krenn:2001,Pecina10} as a means of reducing overgeneration, with the obvious disadvantage of impeding the generation of a broad-coverage multiword lexicon. This is the central challenge addressed in this paper: how to avoid redundancy/computational intractability when selecting a lexicon from a huge collection of $n$-grams, many of which show basic statistical irregularities only as a result of their syntactic properties or because of partial correspondence with a true multiword lexical item. Our solution to this challenge takes the form of an iterative model, where $n$-gram types compete with each other in a lattice to generate a compact, parsimonious vocabulary which best explains the statistics extracted from the corpus. 

A central challenge in building comprehensive multiword lexicons is paring down the huge space of possibilities without imposing restrictions which disregard a major portion of the multiword vocabulary of a language: allowing for diversity creates significant redundancy among statistically promising candidates. The lattice model proposed here addresses this primarily by having the candidates---contiguous and non-contiguous $n$-gram types---compete with each other based on subsumption and overlap relations to be selected as the best (\ie most parsimonious) explanation for statistical irregularities. We test this approach across four large corpora in three languages, including two relatively free-word-order languages (Croatian and Japanese), and find that this approach consistency outperforms alternatives, offering scalability and many avenues for future enhancement.




\section{Background and Related Work}

\label{sec:background}

In this paper we will refer to the targets of our lexicon creation efforts as \termdef{formulaic sequences}, following the terminology of Wray \shortcite{Wray02,Wray08}, wherein a formulaic sequence (FS) is defined as ``a sequence, continuous or discontinuous, of words or other elements, which is, or appears to be, prefabricated: that is, stored and retrieved whole from memory at the time of use, rather than being subject to generation or analysis by the language grammar.'' In this, a FS shows signs of being part of a mental lexicon.\footnote{Though by this definition individuals or small groups may have their own FS, here we are only interested in FS that are shared by a recognizable language community.} As noted by Wray \shortcite{Wray08},  formulaic sequence theory is compatible with other highly multiword, lexicalized approaches to language structure, in particular Pattern Grammar \cite{PG} and Construction Grammar \cite{ConG}; an important distinction, though, is that these sorts of theories often posit entirely abstract grammatical constructions/patterns/frames which do not fit well into the FS framework. Nevertheless, since many such constructions \textit{are} composed of sequences of specific words, the FS inventory of a language includes many flexible constructions (\eg \ex{ask \gap for}) along with entirely fixed combinations (\eg \ex{rhetorical question}) not typically of interest to grammarians. Note that the FS framework allows for individual morphemes to be part of a formulaic sequence, but for practical reasons we focus primarily on lemmatized words as the unit out of which FS are built.

In computational linguistics, the most common term used to describe multiword lexical units is \emph{multiword expression} (``MWE'': \newcite{Sag02}, \newcite{Baldwin10}), but here we wish to make a principled distinction between at least somewhat non-compositional, strongly lexicalized MWEs and FS, a near superset which includes many MWEs but also compositional linguistic formulas. This distinction is not a new one; it exists, for example, in the original paper of \newcite{Sag02} in the distinction between lexicalized and institutionalized phrases, and also to some extent in the MWE annotation of \newcite{Schneider14a}, who distinguish between weak (collocational)\footnote{Here we avoid the term \emph{collocation} entirely due to confusion with respect to its interpretation. Though some define it similarly to our definition of FS, it can be applied to any words that show a statistical tendency to appear in the vicinity of one another for any reason: for instance, the pair of words \ex{doctor/nurse} might be considered a collocation \cite{Ramisch14}.}  and strong (non-compositional) MWEs. It is our contention, however, that separate, precise terminology is useful for research targeted at either class: we need not strain the concept of MWE to include items which do not require special semantics, nor are we inclined to disregard the larger formulaticity of language simply because it is not the dominant focus of MWE research. Many MWE researchers might defensibly balk at including in their MWE lexicons and corpus annotations (English) FS such as \ex{there is something going on}, \ex{it is more important than ever to \dotts}, \ex{\dotts do not know what it is like to \dotts}, \ex{there is no shortage of \dotts}, \ex{the rise and fall of \dotts}, \ex{now is not the time to \dotts}, \etc as well as tens of thousands of other such phrases which, along with less compositional MWEs like \ex{be worth \dotts's weight in gold}, fall under the FS umbrella. Another reason to introduce a different terminology is that there are classes of phrases which are typically considered MWEs that do not fit well into an FS framework, for instance novel compound nouns whose semantics are accessible by analogy (\eg \ex{glass limb}, analogous to \ex{wooden leg}). We also exclude from the definition of both FS and MWE those named entities that refer to people or places that are little-known and/or whose surface form appears derived (\eg \ex{Mrs.\ Barbara W.\ Smith} or \ex{Smith Garden Supplies Ltd}). \figref{fig:terminology} shows the conception of the relationship between FS, (multiword) constructions, MWE, and (multiword) named entities that we assume for this paper.


\begin{figure}[!t]
\center{\includegraphics[width=0.3\textwidth]{FS_diagram_1_2.pdf}}
\caption{Multiword Terminology}
\label{fig:terminology}
\end{figure}



From a practical perspective, the starting point for multiword lexicon creation has typically been lexical association measures \cite{Church90,Dunning93,Schone01,Evert04,Pecina10,DeAraujo11,Kulkarni11,Ramisch14}. When these methods are used to build a lexicon, particular binary syntactic patterns are typically chosen. Only some of these measures generalize tractably beyond two words, for example PMI \cite{Church90}, i.e., the log ratio of the joint probability to the product of the marginal probabilities of the individual words. Another measure that addresses sequences of longer than two words is the $C$-value \cite{Frantzi00} which weights term frequency by the log length of the $n$-gram while penalizing $n$-grams that appear in frequent larger $n$-grams. Mutual expectation \cite{Dias99} involves deriving a normalized statistic that reflects the extent to which a phrase resists the omission of any constituent word. Similarly, the lexical predictability ratio (LPR) of \newcite{Brooke15b} is an association measure applicable to any possible syntactic pattern, which is calculated by discounting syntactic predictability from the overall conditional probability for each word given the other words in the phrase. Though most association measures involve only usage statistics of the phrase and its subparts, the DRUID measure \cite{Riedl15} is an exception which uses distributional semantics around the phrase to identify how easily an $n$-gram could be replaced by a single word.

Typically multiword lexicons are created by ranking $n$-grams according to an association measure and applying a threshold. The algorithm of \newcite{Silva99} is somewhat more sophisticated, in that it identifies the local maxima of association measures across subsuming $n$-grams within a sentence to identify MWEs of unrestricted length and syntactic composition. Its effectiveness beyond noun phrases, however, seems relatively limited \cite{Ramisch12}. Brooke et \al \shortcite{Brooke14a,Brooke15b} developed a heuristic method intended for general FS extraction in larger corpora, first using conditional probabilities to do an initial (single pass) coarse-grained segmentation of the corpus, followed by a pass through the resulting vocabulary, breaking larger units into smaller ones based on a tradeoff between marginal and conditional statistics. The work of \newcite{Newman12} is an example of an unsupervised approach which does not use association measures: it extends the Bayesian word segmentation approach of \newcite{Goldwater09} to multiword tokenization, applying a generative Dirichlet Process model which jointly constructs a segmentation of the corpus and a corresponding multiword vocabulary.

Other research in MWEs has tended to be rather focused on particular syntactic patterns such as verb-noun combinations \cite{Fazly09}. The system of \newcite{Schneider14b} distinguishes a full range of MWE sequences in the English Web Treebank, including gapped expressions, using a supervised sequence tagging model. Though, in theory, automatic lexical resources could be a useful addition to the Schneider et \al model, which uses only manual lexical resources, attempts to do so have achieved mixed success \cite{Riedl16}. 

The motivations for building lexicons of FS naturally overlap with those for MWE: models of distributional semantics, in particular, can benefit from sensitivity to multiword units \cite{Cohen:Widdows:2009}, as can parsing \cite{Constant16} and topic models \cite{Lau+:2012b}. One major motivation for looking beyond MWEs is the ability to carry out broader linguistic analyses. Within corpus linguistics, multiword sequences have been studied in the form of \textit{lexical bundles} \cite{Biber04}, which are simply $n$-grams that occur above a certain frequency threshold. Like FS, lexical bundles generally involve larger phrasal chunks that would be missed by traditional MWE extraction, and so research in this area has tended to focus on how particular formulaic phrases (\eg \textit{if you look at}) are indicative of particular genres (\eg university lectures). Lexical bundles have been applied, in particular, to learner language: for example,  \newcite{Chen10} show that non-native student writers use a severely restricted range of lexical bundle types, and tend to overuse those types, while \newcite{Granger14} investigate the role of proficiency, demonstrating that intermediate learners underuse lower-frequency bigrams and overuse high-frequency bigrams relative to advanced learners. \newcite{Sakaguchi16} demonstrate that improving fluency (closely linked to the use of linguistic formulas) is more important than improving strict grammaticality with respect to native speaker judgments of non-native productions; \newcite{Brooke15b} explicitly argue for FS lexicons as a way to identify, track, and improve learner proficiency.


\section{Method}

Our approach to FS identification involves optimization of the total explanatory power of a lattice, where each node corresponds to an $n$-gram type. The explanatory power of the whole lattice is defined simply as a product of the \termdef{explainedness} of the individual nodes. Each node can be considered either ``on'' (\textit{is an FS}) or ``off'' (\textit{is not an FS}). The basis of the calculation of explainedness is the syntax-sensitive LPR association measure of \newcite{Brooke15b}, but it is calculated differently depending on the on/off status of the node as well as the status of the nodes in its vicinity. Nodes are linked based on $n$-gram subsumption and corpus overlap relationships (see \figref{fig:example}), with ``on'' nodes typically explaining other nodes.  Given these relationships, we iterate over the nodes and greedily optimize the on/off choice relative to explainedness in the local neighborhood of each node, until convergence.

\subsection{Collecting statistics}

The first step in the process is to derive a set of $n$-grams and related statistics from a large, unlabeled corpus of text. Since our primary association measure is an adaption of LPR, our approach in this section mostly follows \newcite{Brooke15b} up until the last stage. An initial requirement of any such method is an $n$-gram frequency threshold, which we set to 1 instance per 10 million words, following \newcite{Brooke15b}.\footnote{Based on manual analysis using the MWE corpus of \newcite{Schneider14a}, this achieves very good (over 90\%) type-level MWE coverage using the frequency filtered $n$-gram statistics from the ICWSM blog corpus (see \secref{sec:evaluation}) after filtering out proper names.} 

We include gapped or non-contiguous $n$-grams in our analysis, in acknowledgment of the fact that many languages have MWEs where the components can be ``separated'', including verb particle constructions in English \cite{Dehe:2002}, and noun-verb idioms in Japanese \cite{Hashimoto:Kawahara:2008}. Having said this, there are generally strong syntactic and length restrictions on what can constitute a gap \cite{Wasow:2002}, which we capture in the form of a language-specific POS-based regular expression (see \secref{sec:evaluation} for details).
This greatly lowers the number of potentially gapped $n$-gram types, increasing precision and efficiency for negligible loss of recall. We also exclude punctuation and lemmatize the corpus, and enforce an $n$-gram count threshold. As long as the count threshold  is substantially above 1, efficient extraction of all $n$-grams can be done iteratively: in iteration $i$, $i$-grams are filtered by the frequency threshold, and then pairs of instances of these $i$-grams with $(i-1)$ words of overlap are found, which derives a set of $(i+1)$-grams which necessarily includes all those over the frequency threshold. 

Once a set of relevant $n$-grams is identified and counted, other statistics required to calculate the \termdef{Lexical Predictability Ratio} (``LPR'') for each word in the $n$-gram are collected. LPR is a measure of how predictable a word is in a lexical context, as compared to how predictable it is given only syntactic context (over the same span of words). Formally, the LPR for word $w_i$ in the context of a word sequence $w_1,..., w_i, ..., w_{n}$ with POS tag sequence $t_1, ..., t_{n}$ is given by:
\begin{displaymath}
\LPR(w_i,w_{1,n}) = \max_{1 \leq j < k \leq n }{\frac{p(w_i|w_{j,k})}{p(w_i|t_{j,k})}}
\end{displaymath}
where $w_{j,k}$ denotes the word sequence $w_j,..., w_{i-1}, w_{i+1}, ..., w_{k}$ excluding $w_{i}$ (similarly for $t_{j,k}$). Note that the lower bound of LPR is 1, since the ratio for a word with no context is trivially 1. We use the same equation for gapped $n$-grams, with the caveat that quantities involving sequences which include the location where the gap occurs are derived from special gapped $n$-gram statistics. Note that the identification of the best ratio across all possible choices of context, not just the largest, is important for longer FS, where the entire POS context alone might uniquely identify the phrase, resulting in the minimum LPR of 1 even for entirely formulaic sequences---an undesirable result. 


In the segmentation approach of \newcite{Brooke15b}, LPR for an entire span is calculated as a product of the individual LPRs, but here we will use the minimum LPR across the words in the sequence:
\begin{displaymath}
\minLPR(w_{1,n}) = \min_{1 \leq i \leq n }{\LPR(w_i,w_{1,n})}
\end{displaymath}
Here, minLPR for a particular $n$-gram does not reflect the \emph{overall} degree to which it holds together, but rather focuses on the word which is its weakest link. For example, in the case of \ex{be keep \gap under wraps} (\figref{fig:example}), a general statistical metric might assign it a high score due to the strong association between \ex{keep} and \ex{under} or \ex{under} and \ex{wraps}, but minLPR is focused on the weaker relationship between \ex{be} and the rest of the phrase. This makes it particularly suited to use in a lattice model of competing $n$-grams, where the choice of \ex{be keep \gap under wraps} versus \ex{keep \gap under wraps} should be based exactly on the extent to which \ex{be} is an essential part of the phrase; the other affinities are, in effect, irrelevant, because they occur in the smaller $n$-gram as well.  


\begin{figure}[!tb]
\center{\includegraphics[width=0.5\textwidth]{FS_diagram_2.pdf}}
\caption{A portion of an $n$-gram lattice. Solid lines indicate subsumption, dotted lines overlaps}
\label{fig:example}
\end{figure}

\subsection{Node interactions}

The $n$-gram nodes in the lattice are directionally connected to nodes consisting of $(n+1)$-grams which subsume them and $(n-1)$-grams which they subsume. For example, as detailed in \figref{fig:example}, the (gapped) $n$-gram \ex{keep \gap under wraps} would be connected ``upwards'' to the node \ex{keep everything under wraps} and connected ``downwards'' to \ex{under wraps}. These directional relationships allow for two basic interactions between nodes in the lattice when a node is turned on: \termdef{covering}, which inhibits nodes below (subsumed by) a turned-on node (\eg if \ex{keep \gap under wraps} is on, the model will tend not to choose \ex{under wraps} as an FS); and \termdef{clearing}, which inhibits nodes above a turned-on node (\eg if \ex{keep \gap under wraps} is on, the model would avoid selecting \ex{keep everything under wraps} as an FS). A third, undirected mechanism is \termdef{overlapping}, where nodes inhibit each other due to overlaps in the corpus (\eg having both \ex{keep \gap under wraps} and \ex{be keep \gap under} as FS will be avoided).


\subsubsection{Covering}
The most important node interaction is \termdef{covering}, which corresponds to discounting or entirely excluding a node due to a node higher in the lattice. Our model includes two types of covering: hard and soft. 

\termdef{Hard covering} is based on the idea that, due to very similar counts, we can reasonably conclude that the presence of an $n$-gram in our statistics is a direct result of a subsuming $(n+i)$-gram. In \figref{fig:example}, \eg if we have 143 counts of \ex{keep \gap under wraps} and 152 counts of \ex{under wraps}, the presence of \ex{keep \gap under wraps} almost completely explains \ex{under wraps}, and we should consider these two $n$-grams as one. We do this by permanently disabling any hard covered node, and setting the minLPR of the covering node to the maximum minLPR among all the nodes it covers (including itself); this means that longer $n$-grams with function words (which often have lower minLPR) can benefit from the strong statistical relationships between open-class lexical features in $n$-grams that they cover. This is done as a preprocessing step, and greatly improves the tractability of the iterative optimization of the lattice. Of course, a threshold for hard covering must be chosen: during development we found that a ratio of $2/3$ (corresponding to a significant majority of the counts of a lower node corresponding to the higher node) worked well.  We also use the concept of hard covering to address the issue of pronouns, based on the observation that specific pronouns often have high LPR values due to pragmatic biases \cite{Brooke15b}; for instance, private state verbs like \ex{feel} tend to have first person singular subjects. In the lattice, $n$-grams with pronouns are considered covered (inactive) unless they cover at least one other node which does not have a pronoun, which allows us to limit FS with pronouns without excluding them entirely: they are included only in cases where they are definitively formulaic. 

\termdef{Soft covering} is used in cases when a single $n$-gram does not entirely account for another, but a turned-on $n$-gram to some extent may explain some of the statistical irregularity of one lower in the lattice. For instance, in \figref{fig:example} \ex{keep \gap under} is not hard-covered by \ex{keep \gap under wraps} (since there are FS such as \ex{keep \gap under surveillance} and \ex{keep it under your hat}), but if \ex{keep \gap under wraps} is tagged as an FS, we nevertheless want to discount the portion of the \ex{keep \gap under} counts that correspond to occurrences of \ex{keep \gap under wraps}, with the idea that these occurrences have already been explained by the longer $n$-gram. If enough subsuming $n$-grams are on, then the shorter $n$-gram will be discounted to the extent that it will be turned off, preventing redundancy. This effect is accomplished by increasing the turned-off explainedness of \ex{keep \gap under} (and thus making turning on less desirable) in the following manner: let $c(\cdot)$ be the count function, $y_i$ the current FS status for node $x_i$ (0 if off, 1 if on) and $ab(x)$ a function which produces the set of indicies of all nodes above node $x$ in the lattice. Then, the $\cover(x_t)$ score for a covered node $t$ is:
\begin{displaymath}
\cover(x_t) = \max\Big(0,\frac{c(x_t) - \sum_{i \in ab(x_t)}y_i\cdot{c(x_i)}}{c(x_t)}\Big)
\end{displaymath}
When applied as an exponent to a minLPR score, it serves as simple, quick-to-calculate approximation of a new minLPR with the counts corresponding to the covering nodes removed from the calculation. The \cover score takes on values in the range 0 to 1, with 1 being the default when no covering occurs.

\subsubsection{Clearing}

In general, covering prefers turning on longer, covering $n$-grams since doing so explains nodes lower in the lattice. Not surprisingly, it is generally desirable to have a mechanism working in opposition, \ie one which views shorter FS as helping to explain the presence of longer $n$-grams which contain them, beyond the FS-neutral syntactic explanation provided by minLPR. \termdef{Clearing} does this by increasing the explainedness of nodes higher in the lattice when a lower node is turned-on. The basic mechanism is similar to covering, except that counts cannot be made use of in the same way---whereas it makes sense to explain covered nodes in proportion to the counts of their covering nodes (since the counts of the covered $n$-grams can be directly attributed to the covering $n$-gram), in the reverse direction this logic fails. 

A simple but effective solution which avoids extra hyperparameters is to make use of the minLPR values of the relevant nodes. In the most common two-node situation, we increase the explainedness of the cleared node based on the ratio of the minLPR of two nodes, though only if the minLPR of the lower node is higher. Generalized to the (rare) case of multiple clearing nodes, we define $\clear(x_t)$ as:
%in this paper, we consider two options: a hard clearing where any turned-on node will clear (set $C_{0}$ to 0 for) any node higher in the lattice which has a minLPR that is lower than it; and a soft 
\begin{displaymath}
\clear(x_t) = \prod_{i \in bl(x_t)}{\min\Big(1,\frac{\minLPR(x_t)}{y_i\cdot\minLPR(x_i)}\Big)}
\end{displaymath}
where $bl(x_t)$ produces a set of indicies of nodes below $x_t$ in the lattice. We refer to this mechanism as ``clearing'' because it tends to clear away a variety of trivial uses of common FS that may have higher LPR due to the lexical and syntactic specificity of the FS. For instance, in \figref{fig:example} if the node \ex{keep \gap under wraps}  is turned on and has a minLPR of 8, then, if the minLPR of a node such as \ex{keep \gap under wraps for} is 4, $\clear(x_t)$ will be 0.5. Like \cover, \clear takes on values in the range 0 to 1, with 1 being the default when no clearing occurs. Note that one major advantage with this particular formulation of clearing is that low-LPR nodes will be unable to clear higher LPR nodes above them in the lattice; otherwise, bad FS like \ex{of the} might be selected as FS based purely to increase the explainedness of the many $n$-grams they appear in.

%We can now define $d_{0}(t)$ as:
%\begin{displaymath}
%d_0(t) = \cover(t) \cdot \clear(t)
%\end{displaymath}
%Note that the effect of both covering and clearing is to increase explainedness for covered/cleared nodes, which indirectly encourages the model to turn an $n$-gram like \ex{keep \gap under wraps} on in order to explain nodes in its vicinity, even supposing its minLPR were relatively low.


\subsubsection{Overlap}

The third mechanism of node interaction involves $n$-grams which overlap in the corpus. In general, independent FS do not consistently overlap. For example, given that \ex{be keep \gap under} and \ex{keep \gap under wraps} often appear together (overlapping on the tokens \ex{keep \gap under}), we do not want both being selected as an FS, even in the case that both have high minLPR. To address this problem, rather than increasing the explainedness of turned-off nodes, we decrease the explainedness of the overlapping turned-on nodes---a penalty rather than an incentive which expresses the model's confusion at having overlapping FS. For non-subsuming nodes $x_i$ and $x_j$, let $\mathit{oc}(x_i,x_j)$ be the count of instances of $x_i$ which contain at least one non-gap token of a corresponding instance of $x_j$. For subsuming nodes, though, overlap is treated asymmetrically, with $\mathit{oc}(x_i,x_j)$ equal to $c(x_j)$ (the lower count) if $j \in ab(x_i)$, but zero if $j \in bl(x_i)$.  Given this definition of $\mathit{oc}$, we define $\overlap(x_t)$ as:
\begin{displaymath}
	\overlap(x_t) = \frac{c(x_t)}{c(x_t) - \sum_{i=1}^{|X|}{y_i\cdot\mathit{oc}(x_t,x_i)}}
\end{displaymath}

Overlap takes on values in the range 1 to $+\infty$, also defaulting to 1 when no overlaps exist. The effect of overlap is hyperbolic: small amounts of overlap have little effect, but nodes with significant overlap will effectively be forced to turn off. 

\subsection{Explainedness}

The objective function maximized by the model is then the explainedness (\expl) across all the nodes of the lattice $X$, $x_i,\dots,x_{N}$, which can be defined in terms of \minLPR, the node interaction functions, and the FS status $y_i$ of each node in the lattice:
%
\begin{multline}
	\expl(X) = \prod_{i=1}^{|X|}{y_i\cdot C^{-\overlap(x_i)}} \\ 
        \hspace*{2mm} + (-y_i + 1)\cdot\minLPR(x_i)^{-\cover(x_i) \cdot \clear(x_i)}
\end{multline}
When a node is off, its explainedness is the inverse of its minLPR, except if there are covering or clearing nodes which explain it by pushing the exponent of minLPR towards zero. When the node is on, its explainedness is the inverse of a fixed cost hyperparameter $C$, though this cost is increased if it overlaps with other active nodes.  All else being equal, when $\minLPR(t) > C$, a node will be selected as an FS, and so, independent of the node interactions, $C$ can be viewed as the threshold for the minLPR association measure under a traditional approach to MWE identification.


%\subsection{Node explainedness functions}

%The central process of the model is to decide which nodes in an $n$-gram lattice correspond to an FS, based on the objective of maximizing the overall explanatory power of the lattice. Like probabilities, explainedness of individual nodes will vary between 0 and 1, with 1 corresponding to a node which is fully explained. We define the explainedness of a node in terms of two functions, capturing the impact on surrounding nodes of a node being either off or on. When a node is off, it is generally desirable (or at least not undesirable) that connected nodes in the lattice are on, since they may serve to partially or entirely explain its existence. When a node is on, however, the presence of other turned-on nodes nearby suggests redundancy, and is not to be encouraged. 

%Let $E_{1}$ be the explainedness of a node when it is on (corresponding to an $n$-gram which has been identified as an FS), and $E_{0}$ be the explainedness of a node when it is off. We calculate $E_{0}$ based on the inverse of minLPR: 
%\begin{displaymath}
%E_0(t) = \minLPR(t)^{-d_0(t)}
%\end{displaymath}
%and $E_{1}$ based on the inverse of a cost parameter $C$:
%\begin{displaymath}
%E_1(t) = C^{-d_1(t)}
%\end{displaymath}
%where $d_0$ and $d_1$ are non-negative functions which capture the lattice context (when there is no context, both default to 1), as detailed in the next section. When the minLPR metric is 1 (corresponding to the case of the syntactic context predicting one of the words of the $n$-gram as well or better than the lexical context), $E_{0}=1$. $E_1$ is equivalent to the initial threshold for building a vocabulary using the minLPR association measure for nodes not influenced by one of the other factors discussed below: all else being equal, when $\minLPR(t) > C$, a node will be selected as an FS. There is no upper bound on $C$, but empirically, we have found a value in the range $[3,6]$ gives reasonable results for the languages presented in this paper.% we note here that there is a fairly small range of good choices for $C$; we have found that setting it too high ($>6$, corresponding to minimum of six times more likely than predicted by POS context alone) will restrict the model primarily to sequences with only open-class words, while setting it too low ($<3$) results in too much random noise.


% The values of $d_0$ and $d_1$ are dependent on the context of the node in the lattice as discussed in the next section, but are both 1 when a node is not influenced by other nodes.


 
\subsection{Optimization}

\begin{algorithm}[t!]
	\caption{Optimization algorithm. $X$ is an ordered list of the nodes in the lattice. Nodes (designated by $x$) contain pointers to the nodes immediately linked to them in the lattice. States (designated by $Y$) indicate whether each node is $\mathrm{ON}$ or $\mathrm{OFF}$. Explainedness values are indicated by $e$. $\mathit{rev}$ = relevant, $\mathit{aff}$ = affected, $\mathit{curr}$ = current}
	\label{alg:opt}
	\begin{algorithmic}
	
	\Function{LocalOpt}{$Y_{\mathit{start}}, x, X_{\mathit{rev}}, X_{\mathit{aff}}$}
		\State $Y_{\mathit{start}} \gets \Call{Set}{Y_{\mathit{start}}, x, \mathrm{ON}}$
		\State $Q \gets \Call{EmptyQueue}{\null}$
		\State $e_{\mathit{best}} \gets 0$
		\State $Y_{\mathit{best}} \gets \mathrm{NULL}$
		\State $\Call{Push}{Q, Y_{\mathit{start}}}$
		\Repeat
			\State $Y_{\mathit{curr}} \gets \Call{Pop}{Q}$
			\State $e_{\mathit{curr}} \gets \Call{CalcExpl}{Y_{\mathit{curr}}, X_{\mathit{aff}}}$
			\For{$x_{\mathit{rev}} \textrm{ in } X_{\mathit{rev}}$}
					\State $Y_{\mathit{new}} \gets \Call{Set}{Y_{\mathit{curr}},x_{\mathit{rev}}, \mathrm{OFF}}$
					\State $e_{\mathit{new}} \gets\Call{CalcExpl}{Y_{\mathit{new}}, X_{\mathit{aff}}}$
					\If{$ e_{\mathit{new}} > e_{\mathit{curr}}$}
				
					\State $\Call{Push}{Q, Y_{\mathit{new}}}$
					\If{$e_{\mathit{new}} > e_{\mathit{best}}}$:
						\State $e_{\mathit{best}} \gets e_{\mathit{new}}$
							\State $Y_{\mathit{best}} \gets Y_{\mathit{new}}$
					\EndIf
					\EndIf
				\EndFor
		\Until{$\Call{IsEmpty}{Q}$}
	\State \Return $Y_{\mathit{best}}$
	\EndFunction
\State
\State $\Call{FrequencySortReverse}{X}$
\State $s \gets \Call{InitializeAllOff}{X}$
\Repeat
	\State $\mathit{Changed} \gets \mathrm{FALSE}$
		\For{$x \textrm{ in } X$}
		\State $X_{\mathit{rev}} \gets \Call{GetRelevant}{x,X}$
		\State $X_{\mathit{aff}} \gets \Call{GetAffected}{X_{\mathit{rev}},X}$
		\State $Y_{\mathit{new}} \gets \Call{LocalOpt}{Y,x,X_{\mathit{rev}},X_{\mathit{aff}}}$
		\If{$Y_{\mathit{new}} \neq Y$}
			\State $Y \gets Y_{\mathit{new}}$
			\State $\mathit{Changed} \gets \mathrm{TRUE}$
		\EndIf
		\EndFor
\Until{$!\mathit{Changed}$}
\end{algorithmic}
\end{algorithm}

The dependence of the explainedness of nodes on their neighbors effectively prohibits a global optimization of the lattice. Fortunately, though most of the nodes in the lattice are part of a single connected graph, most of the effects of nodes on each other are relatively local, and effective local optimizations can be made tractable by applying some simple restrictions. The main optimization loop consists of iterations over the lattice until complete convergence (no changes in the final iteration). For each iteration over the main loop, each potentially active node is examined in order to evaluate whether its current status is optimal given the current state of the lattice. The order that we perform this has an effect on the result: among the obvious options (LPR, $n$-gram length), in development good results were obtained through ordering nodes by frequency, which gives an implicit advantage to relatively common $n$-grams.

Given the relationships between nodes, it is obviously not sufficient to consider switching only the present node. If, for instance, one or more of \ex{be keep \gap under wraps}, \ex{under wraps}, or \ex{be keep \gap under} has been turned on, the covering, clearing, or overlapping effects of these other nodes will likely prevent a competing node like \ex{keep \gap under wraps} from being correctly activated. Instead, the algorithm identifies a small set of ``relevant'' nodes which are the most important to the status of the node under consideration. Since turned-off nodes have no direct effect on each other, only turned-on nodes above, below, or overlapping with the current node in the lattice need be considered.  Once the relevant nodes have been identified, all nodes (including turned-off nodes) whose explainedness is affected by one or more of the relevant nodes are identified. Next, a search is carried out for the optimal configuration of the relevant nodes, starting from an `all-on' state and  iteratively considering new states with one relevant node turned off; the search continues as long as there is an improvement in explainedness. Since the node interactions are roughly cumulative in their effects, this approach will generally identify the optimal state without the need for an exhaustive search. See \algoref{alg:opt} for details.

Omitted from \algoref{alg:opt} for clarity are various low-level efficiencies which prevent the algorithm from reconsidering states already checked or from recalculating the explainedness of nodes when unnecessary. We also apply the following efficiency restrictions, which significantly reduce the runtime of the algorithm. In each case, more extreme (less efficient) values were individually tested using a development set and found to provide no benefit in terms of the quality of the output lexicon:
\begin{compactitem}
\item We limit the total number of relevant nodes to 5. When there are more than 5 nodes turned on in the vicinity of the target node, the most relevant nodes are selected by ranking candidates by the absolute difference in explainedness across possible configurations of the target and candidate node considered in isolation;
\item To avoid having to deal with storing and processing trivial overlaps, we exclude overlaps with a count of less than 5 from our lattice;
\item Many nodes have a minLPR which is slightly larger than 1 (the lowest possible value). There is very little chance these nodes will be activated by the algorithm, and so after applying hard covering, we do not consider activating nodes with $\minLPR < 2$.
\end{compactitem}

\section{Evaluation}
\label{sec:evaluation}

We evaluate our approach across three different languages, including evaluation sets derived from four different corpora selected for their size and linguistic diversity. In English, we follow \newcite{Brooke15b} in using a 890M token filtered portion of the ICWSM blog corpus \cite{ICWSM} tagged with the Tree Tagger \cite{Schmid95}. To facilitate a comparison with \newcite{Newman12}, which does not scale up to a corpus as large as the ICWSM, we also build a lexicon using the 100M token British National Corpus \cite{BNC}, using the standard CLAWS-derived POS tags for the corpus. Lemmatization included removing all inflectional marking from both words and POS tags. For English, gaps are identified using the same POS regex used in \newcite{Brooke15b}, which includes simple nouns and portions thereof, up to a maximum of 4 words.


The other two languages we include in our evaluation are Croatian and Japanese. Relative to English, both languages have freer word order: we were interested in probing the challenges associated with using an $n$-gram approach to FS identification in such languages. For Croatian, we used the 1.2-billion-token fhrWaC corpus \cite{snajder2013building}, a filtered version of the Croatian web corpus hrWaC \cite{ljubesic2014bs}, which is POS-tagged and lemmatized using the tools of \newcite{agic2013lemmatization}. Similar to English, the POS regex for Croatian includes simple nouns, adjectives and pronouns, but also other elements that regularly appear inside FS, including both adverbs and copulas. For Japanese, we used a subset of the 100M-page web corpus of \newcite{Shinzato+:2008}, which was roughly the same token length as the English corpus. We segmented and POS-tagged the corpus with MeCab \cite{Kudo:2008} using the UNIDIC morphological dictionary \cite{Den:2007}. The POS regex for Japanese covers the same basic nominal structures as English, but also includes case markers and adverbials. Though our processing of Japanese includes basic lemmatization related to superficial elements like the choice of writing script and politeness markers, many elements (such as case marking) that are removed by lemmatization in Croatian are segmented into independent morphological units in the MeCab output, making the task somewhat different for the two languages.

\newcite{Brooke15b} introduced a method for evaluating FS extraction without a reference lexicon or direct annotation of the output of a model. Instead, $n$-grams are sampled after applying the frequency threshold and then annotated as being either an FS or not. Benefits of this style of evaluation include replicability, the diversity of FS, and the ability to calculate a true F-score.  We use the annotation of 2000 $n$-grams in the ICWSM corpus from that earlier work, and applied the same annotation methodology to the other three corpora: after training and based on written guidelines derived from the definitions of \newcite{Wray08}, three native-speaker, educated annotators judged 500 contiguous $n$-grams and another 500 gapped $n$-grams for each corpus. 





\begin{table}[!bt]
 
 \begin{center}
	 \begin{tabular}{@{}lc@{\,\,\,}cc@{\,\,\,}cc@{\,\,\,}c@{}}

       \toprule
       & \multicolumn{2}{c}{Contiguous} & \multicolumn{2}{c}{Gapped} & \multicolumn{2}{c}{$\kappa$} \\ %\multirow{2}{*}{$\kappa$}\\
       \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
			& FS & non-FS & FS & non-FS & Pre & Post \\
			 \midrule
			ICWSM    & 169  & 702 & 29 & 916 &0.52 &0.84 \\
			BNC      & \z49 & 403 & \z8& 475 &0.51 &0.84 \\
			Croatian & \z64 & 382 & 11 & 456 & 0.58 &0.87 \\
			Japanese & 102  & 337 & \z9 & 438 & 0.49 &0.82\\
       \bottomrule
 \end{tabular}
  \caption{ Statistics for test sets }
	 \label{tab:stats}

 \end{center}

 \end{table}

Other than the inclusion of new languages, our test sets differ from \newcite{Brooke15b} in two ways. One advantage of a type-based annotation approach, particularly with regards to annotation with a known subjective component, is that it is quite sensible to simply discard borderline cases, improving reliability at the cost of some representativeness. As such we entirely excluded from our test set $n$-grams that just one annotator marked as FS. \tabref{tab:stats} contains the counts for the four test sets after this filtering step and Fleiss' Kappa scores before (``Pre'') and after (``Post'').  The second change is that for the main evaluation we collapsed gapped and contiguous $n$-grams into a single test set. The rationale is that the number of positive gapped examples is too low to provide a reliable independent F-score.

%(see further discussion in \secref{sec:discussion}). 


 \begin{table*}[!bt]
 
 \begin{center}
	 \setlength{\tabcolsep}{5.5pt}
\begin{tabular}{lcccccccccccccccc}

       \toprule
				& \multicolumn{7}{c}{\bf{English}} \\
       \cmidrule(lr){2-8}			
       & \multicolumn{3}{c}{\bf{ICWSM}} & &  \multicolumn{3}{c}{\bf{BNC}} & & \multicolumn{3}{c}{\bf{Croatian}}  && \multicolumn{3}{c}{\bf{Japanese}} \\
       \cmidrule(lr){2-4} \cmidrule(lr){6-8} \cmidrule(lr){10-12} \cmidrule(lr){14-16}
      & P & R & F &   & P & R & F &   & P & R & F &  & P & R & F \\
          \midrule 
Countrank & 0.13& 0.09 & 0.10 && 0.08 & 0.14 & 0.10 && 0.08 & 0.12 & 0.10 && 0.11 & 0.06 & 0.08 \\
PMIrank & 0.24& 0.15& 0.18 & & 0.13 & 0.25 & 0.17 & & 0.21 &0.25 & 0.23 & & 0.18& 0.08 & 0.11 \\ 
minLPRrank & 0.47& 0.33 & 0.38 & & 0.27& 0.40 & 0.32 & & 0.34 & 0.41 & 0.37 &  & 0.53 & 0.26 & 0.35 \\ 

LPR-seg  &0.53 & 0.42 & 0.47 && 0.37 & 0.44 & 0.40 & & 0.41 & 0.47  & 0.43 &  & 0.69 & 0.43 & 0.53 \\ 
  \midrule

	Lattice \emph{$-$cl,ovr} & 0.47& \bf{0.53} & 0.50 & & 0.31& \bf{0.60} & 0.41 & & 0.32 & 0.66 & 0.43 & & 0.49 & 0.61 & 0.54 \\ 

	Lattice \emph{$-$cl} & 0.57& 0.42 & 0.49 & & 0.33& 0.58 & 0.42 & & 0.39 & 0.56 & 0.46 & & 0.63 & 0.49 & 0.55 \\  	 
			
	Lattice \emph{$-$ovr} & 0.52& 0.51 & 0.51 & & 0.34& \bf{0.60} & 0.44& & 0.36 & \bf{0.67} & 0.47 & & 0.53 & \bf{0.60} & 0.56 \\  
			
				Lattice & \bf{0.67} & 0.43 & \bf{0.52} & & \bf{0.40}& \bf{0.60} & \bf{0.48} & &\bf{0.44} & 0.56 & \bf{0.49} & & \bf{0.78} & 0.48 & \bf{0.59} \\ 
            \bottomrule

 \end{tabular}
 \caption{Results of FS identification in various test sets: Countrank = ranking with frequency; PMIrank = PMI-based ranking; minLPRrank = ranking with minLPR; LPRseg = the method of Brooke et \al (2015); ``$-$cl'' = no clearing; ``$-$ovr'' = no penalization of overlaps; ``P'' = Precision; ``R'' = Recall; and ``F'' = F-score. Bold is best in a given column. The performance difference of the Lattice model relative to the best baseline for all test sets considered together is significant at $ p < 0.01$ (based on the permutation test: \protect\newcite{yeh2000more}).}
	 \label{tab:main}

 \end{center}


 \end{table*}


 \begin{table}[!bt]
 
 \begin{center}
	
	 \begin{tabular}{lccc}

       \toprule
			& P & R & F\\
			 \midrule
			PMIrank & 0.20 & 0.29 & 0.23 \\
			minLPRrank & 0.34 & 0.45 & 0.39 \\
			LPR-seg & 0.42 & 0.45 & 0.43 \\
			LocalMaxs & \bf{0.56} & 0.39 & 0.46 \\
			DP-seg & 0.35 & \bf{0.71} & 0.47 \\
  \midrule
			Lattice & 0.47 & 0.63 & \bf{0.54} \\
       \bottomrule
 \end{tabular}
  \caption{ Results of FS identification in contiguous BNC test set; LocalMaxs = method of da Silva and Lopes (1999); DP-seg = method of Newman et \al (2012)}
	\label{tab:BNC}

 \end{center}

 \end{table}	



 Our primary comparison is with the heuristic LPR model of \newcite{Brooke15b}, which is scalable to large corpora and includes gapped $n$-grams. For the BNC, we also benchmark against the DP-seg model of \newcite{Newman12} with recommended settings, and the LocalMaxs algorithm of \newcite{Silva99} using SCP; neither of these methods scale to the larger corpora.\footnote{DP-seg is far too slow, and LocalMaxs, though faster, calculates counts for all $n$-grams in the corpus, which would require terabytes of RAM for the large corpora.} Because these other approaches only generate sequential multiword units, we use only the sequential part of the BNC test set for this evaluation. All comparison approaches have themselves been previously compared against a wide range of association measures. As such, we do not repeat all these comparisons here, but we do consider a lexicon built from ranking $n$-grams, according to the measure used in our lattice (minLPR) as well as PMI and raw frequency. For each of these association measures we rank all $n$-grams above the frequency threshold and build a lexicon equal to the size of the lexicon produced by our model.
%All comparisons here are subject to the same $n$-gram frequency threshold as the main model, since the content of our test sets are also linked to this threshold.

%\footnote{One popular association measure which is theoretically extensible to $n$-grams of any length but which has not been addressed in the context of general MWE/FS identification is the (log-)likelihood ratio \cite{Dunning93}. Calculating a likelihood ratio in the general case involves considering all possible ways an $n$-gram could have been generated statistically from its component words: that is, whereas minLPR only considers the addition of one word to each $(n-1)$-gram, likelihood ratios must consider all combinations of all component $(n-1,2,3\ldots)$-grams, including, in our case, not only contiguous but also non-contiguous components. The corresponding exponential increase in complexity renders it and other such methods impractical for comprehensive FS identification.}


We created small development sets for each corpus and used them to do a thorough testing of parameter settings. Although it is generally possible to increase precision by increasing $C$, we found that across corpora we always obtained near-optimal results with $C=4$, so to demonstrate the usefulness of the lattice technique as an entirely off-the-shelf tool, we present the results using identical settings for all four corpora. We treat covering as a fundamental part of the Lattice model, but to investigate the efficacy of other node interactions within the model we, present results with overlap and clearing node interactions turned off.
		

\section{Results}

The main results for FS acquisition across the four corpora are shown in \tabref{tab:main}. As noted in \secref{sec:background}, simple statistical association measures like PMI do poorly when faced with syntactically-unrestricted $n$-grams of variable length: minLPR is clearly a much better statistic for this purpose. The LPRseg method of \newcite{Brooke15b} consistently outperforms simple ranking, and the lattice method proposed here does better still, with a margin that is fairly consistent across the languages. Generally, clearing and overlap node interactions provide a relatively large increase in precision at the cost of a smaller drop in recall, though the change is fairly symmetrical in Croatian.  When only covering is used, the results are fairly similar to \newcite{Brooke15b}, which is unsurprising given the extent to which decomposition and covering are related. The Japanese and ICWSM corpora have relatively high precision and low recall, whereas both the BNC and Croatian corpora have low precision and high recall.

In the contiguous FS test set for the BNC (\tabref{tab:BNC}), we found that both the LocalMaxs algorithm and the DP-seg method of \newcite{Newman12} were able to beat our other baseline methods with roughly similar F-scores, though both are well below our Lattice method. Some of the difference seems attributable to fairly severe precision/recall imbalance, though we were unable to improve the F-score by changing the parameters from recommended settings for either model.


\section{Discussion} \label{sec:discussion}


\begin{table*}[!bt]
 
 \begin{center}
 \begin{tabular}{lccccccc}
\toprule
& & & & \multicolumn{4}{c}{\bf{By Length (\%) }} \\
       \cmidrule(lr){5-8}	
\bf{Lexicon} & \bf{Word types} & \bf{POS types} & \bf{Gapped} (\%) & 2 & 3 & 4 & 5+ \\ 
\midrule
English ICWSM & 202k & 16.8k & 18.6 & 47.1 & 30.9 & 14.8 & \z7.2  \\
English BNC & 367k &  29.9k& 26.2& 38.3 & 36.3 & 17.9 & \z7.5 \\
Croatian & 169k & \z6.6k & \z8.1& 56.3& 30.8 & 9.9 & \z3.0 \\
Japanese & 219k &  29.1k &\z7.9& 39.6 & 31.0& 16.9 & 12.5 \\
\bottomrule
\end{tabular}
  \caption{Statistics for the lexicons created by our lattice method}
	 \label{tab:lexstats}

 \end{center}


 \end{table*}

%\subsection{Overall Results}

Though the results across the four corpora are reasonably similar with respect to overall F-score, there are some discrepancies. By using the standard UNIDIC morpheme representation as the base unit for Japanese, the model ends up doing an extra layer of FS identification, one which is provided by word boundaries in the other languages. The result is that there are relatively more FS for Japanese: precision is high, and recall is comparably low.  Importantly, the initial $n$-gram statistics actually reflect that Japanese is different: the number of $n$-gram types over length 4 is almost twice the number in the ICWSM corpus. One idea for future work is to automatically adapt to the input language/corpus in order to ensure a good balance between precision and recall.

At the opposite extreme, the low precision of the BNC is almost certainly due to its relatively small size: whereas the $n$-gram threshold we used here results in minimum counts of roughly 100 for the other three corpora, the BNC statistics include $n$-grams with counts of less than 10. At such low counts, LPR is less reliable and more noise gets into the lexicon: the first column of \tabref{tab:lexstats} shows that the BNC is noticeably larger then the other lexicons, and the higher numbers in columns 2 and 3 (number of POS types and percentage of gapped expressions, respectively) are also indicative of increased noise.  This  could be resolved by increasing the $n$-gram threshold. It might also make sense to simply avoid smaller corpora, though for some applications a smaller corpus may be unavoidable. One idea we are pursing is modifying the calculation of the LPR metric to use a more conservative  probability estimate than maximum likelihood in the case of low counts.

We were interested in Croatian and Japanese in part because of their relatively free word order, and whether the handling of gaps would help with identifying FS in these languages. We discovered, however, that free word order actually results in \emph{more} of a tendency towards contiguous FS, not less, a fact that is reflected in our test sets (\tabref{tab:stats}) as well as the lexicons themselves (\tabref{tab:lexstats}). Strikingly rare in Croatian, in particular, are expressions where the content of a gap is an argument which must be filled to syntactically complete an expression: it is English whose fixed-word-order constraints often keep elements of an FS distant from each other. The gaps that do happen in Croatian are mostly prosody-driven insertions of other elements into already complete FS. This phenomena highlights a problem with the current model, in that gapped and contiguous versions of the same $n$-gram sequence (\eg \ex{take away} and \ex{take \gap away}) are, at present, considered entirely independently. Alternatives for dealing with this include collapsing statistics to create a single node in the lattice, creating a promoting link between contiguous and gapped versions of the same $n$-grams sequence in the lattice model, or switching to a dependency representation (which, we note, requires very little change to the basic model presented here, but would narrow its applicability). 


The statistics in \tabref{tab:lexstats} otherwise reflect the quantity and diversity of FS across the corpora, particularly in terms of the number of POS patterns represented in the lexicon. Looking at the most common POS patterns across languages, only noun-noun and adjective-noun combinations ever account for more than 5\% of all word types in any of the lexicons. Though some of the diversity can, of course, be attributed to noise, it is safe to say that most FS do not fall into the standard two-word syntactic categories used in MWE work, and therefore identifying them requires a much more general approach like the one presented here.

\begin{table*}[!t]
 
 \begin{center}
 \begin{tabular}{p{1.5cm}p{14cm}}
\toprule
English (ICWSM) & \ex{heart ache}, \ex{so \gap have some time}, \ex{part of the blame}, \ex{via slashdot}, \ex{any more questions}, \ex{protein expression}, \ex{work in \gap bank}, \ex{al-qaeda terrorist}, \ex{continue discussions}, \ex{speak about * issue}\\
\midrule
English (BNC) & \ex{go into decline}, \ex{Maureen says}, \ex{be open to all * in the}, \ex{Peggy Sue}, \ex{square \gap shoulders}, \ex{delivery system for}, \ex{this \gap also includes}, \ex{license endorsed}, \ex{point \gap finger}, \ex{highly \gap asset}\\
\midrule
Croatian & \ex{negativno utjecati na} \gl{negatively affects on},
% TJB: awkard gloss
\ex{jedan od dobrih poznavatelja} \gl{one of the best connoisseurs of}, \ex{jasno \gap je da} \gl{it is clear to \gap that}, \ex{promet teretnih vozila} \gl{good vehicle traffic}, \ex{odvratiti pozornost} \gl{divert attention}, \ex{biti general poslije bitke} \gl{be the general after the battle}, \ex{popularni internetski} \gl{popular internet}, \ex{izazvati kaos} \gl{cause chaos}, \ex{austrijski investitor} \gl{Austrian investor}, \ex{ideja o gradnji} \gl{the idea of building} \\
\midrule
Japanese & \begin{CJK*}{UTF8}{min} \ex{\ \ } \gl{highway construction}, \ex{\ } \gl{the second half of the fiscal year}, \ex{\ \ \ } \gl{temporary labor agency}, \ex{\ \ \gap\ \ } \gl{repeat occurrences of \gap like this},  \ex{\ \ } \gl{cold sufferer}, \ex{\ } \gl{DHCP server}, \ex{\ } \gl{first half comparison}, \ex{\ \  } \gl{examination of administrative affairs}, \ex{\ \ } \gl{own writing}, \ex{\ } \gl{deep flavor} \end{CJK*} \\
\bottomrule
\end{tabular}
  \caption{10 randomly-selected examples from the final FS lexicon from each corpus. Lemmas have been converted to inflected forms where appropriate for readability.}
	 \label{tab:sample}

 \end{center}

\end{table*}


%The fact that less than 20\% of the FS derived from the BNC appear in the lexicon for the ICWSM corpus is fairly telling of how important genre differences are to the range of FS that will be seen in a particular corpus.
\tabref{tab:sample} contains 10 randomly selected examples from each of the lexicons produced by our method. Among the English examples, most of the clear errors are bigrams that reflect particular biases of their respective corpora: The phrase \ex{via slashdot} comes from boilerplate text identifying the source of an article, whereas \ex{Maureen} (from \ex{Maureen says}) is a character in one of the novels included in the BNC. The longer FS mostly seem sensible, in that they are plausible lexicalized constructions, though \ex{be open to all * in the} from the BNC seems too long and is likely the result of noise due to insufficient examples. Some FS are dialectal variants, for instance \ex{license endorsed} refers to British traffic violations. More generally, the FS lexicons created by these two corpora are quite distinct, sharing less than 50\% of their entries.

One striking thing about the non-English FS is how poorly they translate: many good FS in these languages become extremely awkward when translated into English. This is expected, of course, for idioms like \ex{biti general poslije bitke} \gl{be the general after the battle} (\ie \gl{hindsight is 20/20}), but it extends to other relatively compositional  constructions like 
\begin{CJK*}{UTF8}{min} \ex{\ \ \gap\ \ } \end{CJK*} \gl{repeat occurrences of \gap like this} and 
\begin{CJK*}{UTF8}{min} \ex{\ } \end{CJK*} 
\gl{first half comparison}. This highlights the potential importance of focusing on FS when learning a language. Though some of the errors seem to be the result of extra material added to a good FS, for instance \ex{promet teretnih vozila} \gl{good vehicle traffic}, most, again, are somewhat inexplicable artifacts of the corpus they were built from, like \ex{austrijski investitor} \gl{Austrian investor}.

Since Zipfian frequency curves naturally extend to multiword vocabulary, our lexicons (and type-based evaluation of them) are, of course, dominated by rarer terms. This is not, we would argue, a serious drawback, since in practical terms there is very little value in focusing on common FS like \ex{of course} which manually-built lexicons already contain; most of the potential in automatic extraction comes from the long tail. However, we did investigate the other end of the Zipfian curve by extracting the 20 most common MWEs (including both strong and weak) from the \newcite{Schneider14a} corpus. In the ICWSM lexicon, our recall for these common terms was fairly high (0.7), with errors mostly resulting from longer phrases containing these terms ``winning out'' (in the lattice) over shorter phrases, which have relatively low LPR due to extremely common constituent words; for example, we missed \ex{on time}, but had 19 FS which contain it (e.g.\ \ex{right on time}, \ex{show up on time}, and \ex{start on time}). In one case that showed this same problem, \ex{waste * time}, the lexicon did have its ungapped version, highlighting the potential for improved handling of this issue. 

%\subsection{FS vs.\ MWE}


In \secref{sec:background}, we noted that FS is generally a much broader category than MWE, which we take as referring to terms which carry significant non-compositional meaning. We decided to investigate the distinction at a practical level by annotating the positive examples in the ICWSM test set for being MWE or non-MWE FS.\footnote{The set was exhaustively annotated by two native-speaker annotators ($\kappa = 0.73$), and conflicts were resolved through discussion.} First, we note that only 28\% of our FS types were labeled MWE; this is in contrast to, for instance, the annotation of \newcite{Schneider14a} where ``weak'' MWE make up a small fraction of MWE types. Even without any explicit representation of compositionality, our model did much better at identifying MWE FS than non-MWE FS:  0.7 versus 0.32 recall. This may simply reflect, however, the fact that a disproportionate number of MWEs were noun-noun compounds, which are fairly easy for the model to identify.

Due to the lack of spaces between words and an agglutinative morphology, the standard approach to tokenization and lemmatization in Japanese involves morphological rather than word segmentation. In terms of the content of the resulting lexicon we believe the effect of this difference on FS extraction is modest, since much of the extra FS in Japanese would simply be single words in other languages (and considered trivially part of the FS lexicon). However, from a theoretical perspective we might very much prefer to build FS for all languages starting from morphemes rather than words. Such a framework could, for instance, capture inflectional flexibility versus fixedness directly in the model, with fixed inflectional morphemes included as a distinct element of the FS and flexible morphemes becoming gaps. However, for many languages this would result in a huge blow up in complexity with only modest increases in the scope of FS identification. Though it is indisputable that inflectional fixedness is part of the lexical information contained in an FS, in practice this sort of information can be efficiently derived post hoc from the corpus statistics.

Though we have demonstrated that competition within a lattice is a powerful method for the production of multiword lexicons, its usefulness derives less from the specific choices we have made in this instantiation of the model, and more from the flexiblity that such a model provides for future research. Not only do alternatives like DP-seg and LocalMaxs fail to scale up to large corpora, there are few obvious ways to improve on their simple underlying algorithms without compromising their elegance and worsening tractability. Fast and functional, the LPR decomp approach is nevertheless algorithmically ungainly, involving multiple layers of heuristic-driven filtering with no possibility of correcting errors. Our lattice method is aimed at something between these extremes: a practical, optimizable model, but with various component heuristics that can be improved upon. For instance, though the current version of clearing is effective and has practical advantages relative to simpler options that we tested, it could be enhanced by more careful investigation of the statistical properties of $n$-grams which contain FS.

We can also consider adding new types of terms to the exponents of the two parts of our objective function, analagous to the cover, clear, and overlap functions, based on other relationships between nodes in the lattice. One type that we have considered is adding new connections between identical or similar syntactic patterns, which could serve to encourage the model to generalize. In English, for instance, it might learn that verb-particle combinations are generally likely to be FS, whereas verb-determiner combinations are not. Our initial investigations suggest, however, it may be difficult to apply this idea without merely amplifying existing undesirable biases in the LPR measure. Bringing in other information such as simple distributional statistics might help the model identify non-compositional semantics, and could, in combination with the existing lattice competition, focus the model on MWEs which could provide a reliable basis for generalization.

For all four corpora, the lattice optimization algorithm converged within 10 iterations.  Although the optimization of the lattice is several orders of magnitude more complex than the decomposition heuristics of \newcite{Brooke15b}, the time needed to build and optimize the lattice is a fraction of the time required to collect the statistics for LPR calculation, and so the end-to-end runtimes of the two methods are comparable. In the BNC, the full lattice method was much faster than LocalMaxs and DP-Seg, though direct runtime comparisons to these methods are of modest value due to differences in both scope and implementation.


%Though in general using the sampling methodology to build test sets has significant benefits in terms of providing a robust, replicable evaluation, F-scores become unreliable when there are very few positive examples, such as with most of our gapped $n$-gram test sets. To do a reliable, focused evaluation of FS with gaps, we would need many more examples than can be produced with straightforward sampling. One option we might consider is adding additional filtering to the sampling processes, for instance using the LPR metric to exclude large numbers of unpromising $n$-grams.

Finally, though the model presented here was designed specifically for FS extraction, we note that it could be useful for related tasks such as unsupervised learning of morphological lexicons, particularly for agglutinative languages. Character or phoneme $n$-grams could compete in an identically structured lattice to be chosen as the best morphemes for the language, with LPR adapted to use phonological predictability (\ie based on vowel/consonant ``tags'') instead of syntactic predictability. It is likely, though, that further algorithmic modifications would be necessary to target morphological phenomena well, and we leave this for future work.

\section{Conclusion}

We have presented here a new methodology for acquiring comprehensive multiword lexicons from large corpora, using competition in an $n$-gram lattice. Our evaluation using annotations of sampled $n$-grams shows that it consistently outperforms alternatives across several corpora and languages. A tool which implements the method, as well as the acquired lexicons, annotation guidelines, and test sets have been made available.\footnote{\url{https://github.com/julianbrooke/LatticeFS}}

\section*{Acknowledgements}

The second author was supported by an Endeavour Research Fellowship
from the Australian Government, and in part by the Croatian Science
Foundation under project UIP-2014-09-7312. We would also like to thank
our English, Japanese, and Croatian annotators, and the TACL reviewers and editors for helping shape this paper into its current form.

\bibliography{mybib}
\bibliographystyle{acl2012}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
